{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a419baa",
   "metadata": {},
   "source": [
    "# 2. Leptons Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7746c13",
   "metadata": {},
   "source": [
    "First, we import the libraries for reading and manipulating our ROOT files. We parametrze the analysis with path to the data, the name of the column (obtained from the CERN page of the datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0102049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# List of the ROOT files downloaded.\n",
    "DATA_FILES = {\n",
    "    \"DoubleMuon_B\": \"../../data/12365/Run2012B_DoubleMuParked.root\",\n",
    "    \"DoubleMuon_C\": \"../../data/12366/Run2012C_DoubleMuParked.root\",\n",
    "    \"DoubleElectron_B\": \"../../data/12367/Run2012B_DoubleElectron.root\",\n",
    "    \"DoubleElectron_C\": \"../../data/12368/Run2012C_DoubleElectron.root\"\n",
    "}\n",
    "\n",
    "# Number of events to load\n",
    "MAX_EVENTS = 300000\n",
    "\n",
    "try:\n",
    "    import uproot # For reading CERN ROOT files\n",
    "    import vector # For fast, correct Lorentz Vector calculations\n",
    "    import awkward as ak # For manipulating variable-length lists\n",
    "    print(\"Success: uproot, vector, and awkward are loaded.\")\n",
    "\n",
    "    # Necessary branches (columns) for each lepton type.\n",
    "    MUON_BRANCHES = [\"event\", \"Muon_pt\", \"Muon_eta\", \"Muon_phi\", \"Muon_mass\", \"Muon_charge\", \"Muon_pfRelIso03_all\"]\n",
    "    ELECTRON_BRANCHES = [\"event\", \"Electron_pt\", \"Electron_eta\", \"Electron_phi\", \"Electron_mass\", \"Electron_charge\", \"Electron_pfRelIso03_all\"]\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Error: uproot, vector, or awkward not found. Please install these libraries to continue.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1164a",
   "metadata": {},
   "source": [
    "The next function  primary goal is to transform our data into a **flat table** where each row represents a single lepton.\n",
    "\n",
    "### 1. Data Type Determination (The Dispatcher)\n",
    "\n",
    "The function first acts as a **dispatcher**, determining the required physics schema based on the input file's identifier (`file_key`):\n",
    "\n",
    "It checks for `\"DoubleMuon\"` or `\"DoubleElectron\"` in the `file_key`. This is a way to get the **lepton flavor** ($\\mu$ or $e$) and select the correct set of **TTree branches** (e.g., `Muon_pt` vs. `Electron_pt`).\n",
    "\n",
    "Then we set the `lepton_prefix` (e.g., `\"Muon\"`) and the **PDG ID** (`flavor_pdg`, 13 or 11), ensuring the output DataFrame is properly annotated.\n",
    "\n",
    "### 2. Event Array Retrieval (The Reader)\n",
    "\n",
    "Using the `uproot` library, the function streams data from the ROOT file's `Events` TTree: It only reads the necessary branches (defined by `MUON_BRANCHES` or `ELECTRON_BRANCHES`), minimizing I/O and memory overhead.\n",
    "\n",
    "The data is loaded into an **Awkward Array (`ak`)**. This choice is crucial because it inherently handles the **variable-length lists** of objects (leptons) within each eventâ€”the \"jagged\" structure.\n",
    "\n",
    "### 3. Flattening and Event ID Reconstruction (The Core Transformation)\n",
    "\n",
    "This is the central logic, converting the Awkward Array into a flat, tabular format ready for Pandas.\n",
    "\n",
    "It calculates `lepton_counts` by getting the length of the list of $p_T$ values (`ak.num(raw_data[f'{lepton_prefix}_pt'])`) for each event. This count is essential for the row-wise mapping.\n",
    "\n",
    "Then, the scalar **`event` ID** is converted to a NumPy array and  `np.repeat(event_ids_np, lepton_counts_np)` is used to **duplicate the event ID** for every lepton it contains. If event 123 has 3 leptons, the ID 123 appears 3 times. This creates the primary key for the flat table.\n",
    "\n",
    "For every required branch (`pt`, `eta`, `iso`, etc.):\n",
    "    1.  `ak.flatten(...)` converts the jagged array (list of lists) into a single, contiguous 1D array of particle properties.\n",
    "    2.  `ak.to_numpy(...)` extracts the raw values, preparing them for the Pandas DataFrame construction.\n",
    "\n",
    "### 4. Final Data Frame Assembly\n",
    "\n",
    "The collected 1D arrays are assembled into the final Pandas DataFrame (`df`).\n",
    "\n",
    "It explicitly iterates over kinematic columns (`pt`, `eta`, etc.) to enforce a uniform `np.float64` data type. This is good practice to prevent downstream issues with vector arithmetic or type mismatches when concatenating multiple files.\n",
    "\n",
    "The pre-determined `flavor_pdg` (13 or 11) is appended as a new column, to provide a permanent **particle identification** tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_from_file(file_path, file_key, max_events):\n",
    "    \"\"\"\n",
    "    Loads specific lepton data (Muon or Electron) based on the trigger file type (file_key).\n",
    "    Returns a flattened DataFrame for that file.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Determine the branches to load based on the trigger type\n",
    "    if (\"DoubleMuon\" in file_key) or (\"4mu\" in file_key):\n",
    "        lepton_prefix = \"Muon\"  # Capitalized to match branch names\n",
    "        branches = MUON_BRANCHES\n",
    "        flavor_pdg = 13 # Muon\n",
    "    elif (\"DoubleElectron\" in file_key) or (\"4e\" in file_key):\n",
    "        lepton_prefix = \"Electron\" # Capitalized to match branch names\n",
    "        branches = ELECTRON_BRANCHES\n",
    "        flavor_pdg = 11 # Electron\n",
    "    else:\n",
    "        print(f\"Warning: Unrecognized file type ({file_key}). Skipped.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Open the ROOT file and read the 'Events' tree (TTree)\n",
    "        with uproot.open(file_path) as file:\n",
    "            tree = file[\"Events\"]\n",
    "\n",
    "            # Read only the necessary branches for this file\n",
    "            raw_data = tree.arrays(\n",
    "                branches,\n",
    "                entry_stop=max_events,\n",
    "                library=\"ak\"\n",
    "            )\n",
    "            print(f\"Step 1: Raw Data read OK\")\n",
    "\n",
    "            # --- CONVERSION TO 'FLATTENED' FORMAT (CORRECTED) ---\n",
    "\n",
    "            # Initialize the dictionary for the flat DataFrame\n",
    "            flattened_data = {}\n",
    "\n",
    "            # 1. Handle Event ID (Repeat the Event ID for each lepton it contains)\n",
    "            event_ids = raw_data['event']\n",
    "\n",
    "            # We use the pT branch to determine the number of leptons per event\n",
    "            lepton_counts = ak.num(raw_data[f'{lepton_prefix}_pt'])\n",
    "\n",
    "            # Robust conversion to NumPy for np.repeat\n",
    "            lepton_counts_np = ak.to_numpy(lepton_counts).astype(np.int64)\n",
    "            event_ids_np = ak.to_numpy(event_ids).astype(np.int64)\n",
    "\n",
    "            # Repeat the Event ID as many times as there are leptons\n",
    "            flattened_data['event_id'] = np.repeat(event_ids_np, lepton_counts_np)\n",
    "\n",
    "            # Diagnostic\n",
    "            print(f\"Diagnostic: Read {len(raw_data)} events. Number of {lepton_prefix}s in the first event: {lepton_counts_np[0] if len(lepton_counts_np) > 0 else 0}\")\n",
    "\n",
    "            # 2. Flatten the lepton data (pt, eta, phi, etc.)\n",
    "            # We don't loop because renaming is complex with capitalization.\n",
    "\n",
    "            # Kinematics\n",
    "            flattened_data['pt'] = ak.to_numpy(ak.flatten(raw_data[f'{lepton_prefix}_pt']))\n",
    "            flattened_data['eta'] = ak.to_numpy(ak.flatten(raw_data[f'{lepton_prefix}_eta']))\n",
    "            flattened_data['phi'] = ak.to_numpy(ak.flatten(raw_data[f'{lepton_prefix}_phi']))\n",
    "            flattened_data['mass'] = ak.to_numpy(ak.flatten(raw_data[f'{lepton_prefix}_mass']))\n",
    "            flattened_data['charge'] = ak.to_numpy(ak.flatten(raw_data[f'{lepton_prefix}_charge']))\n",
    "\n",
    "            # Isolation\n",
    "            iso_key = f'{lepton_prefix}_pfRelIso03_all'\n",
    "            flattened_data['iso'] = ak.to_numpy(ak.flatten(raw_data[iso_key]))\n",
    "\n",
    "            df = pd.DataFrame(flattened_data)\n",
    "            print(f\"Step 2: Flatten OK\")\n",
    "\n",
    "            # --- KEY CORRECTION: ENFORCE NUMERIC TYPING FOR 'VECTOR' ---\n",
    "            kinematic_cols = ['pt', 'eta', 'phi', 'mass']\n",
    "            for col in kinematic_cols:\n",
    "                # Ensure kinematic columns are of standard float64 type\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "            # Add the flavor identification column (PDG ID)\n",
    "            df['flavor'] = flavor_pdg\n",
    "            print(f\"Step 3: ID OK\")\n",
    "\n",
    "            for col in kinematic_cols:\n",
    "                print(f\"DEBUG: Type/Dtype of df['{col}'].values: {type(df[col].values)} / {df[col].values.dtype}\")\n",
    "\n",
    "            print(f\"Success: {len(df)} leptons loaded from {file_key}.\")\n",
    "            return df\n",
    "\n",
    "    except Exception as e:\n",
    "        # Leave a clearer message for the user\n",
    "        print(f\"\\nERROR: Could not load file {file_path}. Does the file exist and contain an 'Events' TTree?\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31644d",
   "metadata": {},
   "source": [
    "This function acts as a **Data Aggregator and Orchestrator**. Its job is to manage the loading of **multiple data files** and combine them into a single, unified DataFrame for subsequent analysis.\n",
    "\n",
    "### 1. Initialization and Orchestration\n",
    "\n",
    "* **Initialization:** It starts with an empty list, `all_dfs`, which will temporarily hold the processed DataFrame from each individual file.\n",
    "* **The Input Map:** It receives `data_files_map`, which is typically a Python dictionary where the **keys** describe the type of data (e.g., \"DoubleMuon\\_RunB\") and the **values** are the physical paths to the ROOT files.\n",
    "* **Looping through Files:** It iterates through this map, treating each file independently.\n",
    "### 2. File Handling and Delegation\n",
    "\n",
    "For every existing file, it calls the crucial **`load_data_from_file`** function above. If `load_data_from_file` returns a DataFrame that is **not empty** (`if not df.empty:`), the function appends this successfully loaded and flattened DataFrame to the `all_dfs` list.\n",
    "### 3. Final Aggregation (Concatenation)\n",
    "\n",
    "Once the loop is finished, the function brings all the pieces together, using **`pd.concat(all_dfs, ignore_index=True)`**.\n",
    "    \n",
    "This takes the list of DataFrames (each containing only one type of lepton, like Muons from Run B, Muons from Run C, etc.) and stacks them vertically to create a single, massive **`final_df`**.\n",
    "\n",
    "`ignore_index=True` ensures the row indexing is reset from 0 to N across the entire combined data set, preventing duplicate index values from the individual files.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_flatten_data(data_files_map, max_events):\n",
    "    \"\"\"\n",
    "    Loads data from all specified files and concatenates them into one DataFrame.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "\n",
    "    print(f\"Attempting to load a maximum of {max_events} events per file...\")\n",
    "    for key, file_name in data_files_map.items():\n",
    "        if os.path.exists(file_name):\n",
    "            df = load_data_from_file(file_name, key, max_events)\n",
    "            if not df.empty:\n",
    "                all_dfs.append(df)\n",
    "        else:\n",
    "            print(f\"WARNING: File not found '{file_name}'. Skipped. Check the path.\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"Loading failed: No data file was successfully loaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all loaded DataFrames\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"\\nTotal loading successful. Total of {len(final_df)} leptons ready for analysis.\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632adf8",
   "metadata": {},
   "source": [
    "The next function performs the essential **initial filtering** of the raw lepton data. Its purpose is to discard background particles and poorly measured objects to get only the high-quality leptons suitable for reconstructing signal processes.\n",
    "\n",
    "### Transverse Momentum ($p_T$)\n",
    "\n",
    "Most low-$p_T$ leptons are from instrumental noise, so we apply a threshold of $\\mathbf{p_T > 5.0}$ **GeV** by creating a new DataFrame (`df_pt_filtered`) containing only rows where the value in the `'pt'` column is greater than $5.0$.\n",
    "\n",
    "### Pseudorapidity ($\\eta$)\n",
    "\n",
    "Detectors like CMS and ATLAS are designed with specific angular coverages. Most high-quality measurements occur in the **central region** (low $|\\eta|$). So we apply a threshold of $\\mathbf{|\\eta| < 2.5}$ by filtering the previous result (`df_pt_filtered`).\n",
    "\n",
    "### Isolation Cut\n",
    "\n",
    "The signal of interest is **prompt leptons** (leptons produced directly from the hard collision or a short-lived particle like the Higgs). A key background source is leptons produced within **hadronic jets** (non-prompt).\n",
    "\n",
    "**Isolation** (`iso`) is a quantity (typically a ratio) that measures the amount of energy deposited around the lepton track. Prompt leptons are **isolated** (low energy nearby), while leptons inside a jet are **non-isolated** (high energy nearby). It applies a threshold of $\\mathbf{iso < 0.3}$.\n",
    "\n",
    "The final DataFrame is returned after calling **`.reset_index(drop=True)`**. This is important because the filtering steps leave gaps in the DataFrame index. Resetting the index ensures a contiguous sequence of row numbers from $0$ to $N-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7260e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quality_cuts(df):\n",
    "    \"\"\"\n",
    "    Applies the minimal quality and kinematic cuts to individual leptons.\n",
    "    This is the first essential filtering step in the H -> 4l analysis.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "\n",
    "    if initial_count == 0:\n",
    "        print(\"The DataFrame is empty, no quality cuts applied.\")\n",
    "        return df\n",
    "\n",
    "    # A. KINEMATIC CUT ON PT (Transverse Momentum)\n",
    "    pt_cut = 5.0\n",
    "    df_pt_filtered = df[df['pt'] > pt_cut]\n",
    "    print(f\"-> pT cut > {pt_cut} GeV: {initial_count - len(df_pt_filtered)} leptons rejected.\")\n",
    "\n",
    "    # B. ACCEPTANCE CUT ON ETA (Pseudorapidity)\n",
    "    eta_max = 2.5\n",
    "    df_eta_filtered = df_pt_filtered[np.abs(df_pt_filtered['eta']) < eta_max]\n",
    "    print(f\"-> |eta| cut < {eta_max}: {len(df_pt_filtered) - len(df_eta_filtered)} leptons rejected (after pT).\")\n",
    "\n",
    "    # C. ISOLATION CUT (Rejecting Leptons from Jets)\n",
    "    iso_cut = 0.3\n",
    "    df_final = df_eta_filtered[df_eta_filtered['iso'] < iso_cut]\n",
    "    print(f\"-> Isolation Cut (iso < {iso_cut}): {len(df_eta_filtered) - len(df_final)} leptons rejected (after pT and eta).\")\n",
    "\n",
    "    final_count = len(df_final)\n",
    "    print(f\"\\nTotal leptons after quality cuts: {final_count}\")\n",
    "    return df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a190214",
   "metadata": {},
   "source": [
    "This function is a **Data Validation and Sanitation** step, critical for ensuring the mathematical integrity of the physics data before complex using `vector` library.\n",
    "\n",
    "### Unphysical Mass Values\n",
    "In physics data processing, sometimes the reconstruction algorithm (especially when dealing with charged particles) can result in a **negative mass value**. Since mass ($m$) is derived from the energy ($E$) and momentum ($p$) via $E^2 - p^2 = m^2$, numerical precision errors can occasionally lead to an unphysical $m^2 < 0$. This is mathematically impossible for real particles.\n",
    "\n",
    "We creates a `negative_mass_mask`, to identify all rows where `df['mass'] < 0`.\n",
    "\n",
    "Instead of discarding the entire lepton, we set the negative masses to a small positive value, **0.1** (instead of $0.0$ to ensure numerical stability in later calculations). This is a pragmatic approach: we correct the mathematical error while preserving the valid $p_T$, $\\eta$, and $\\phi$ measurements.\n",
    "\n",
    "### General Validity and Finite Check\n",
    "\n",
    "The second part of the function focuses on the strict requirement that all four kinematic variables are numerically well-defined.\n",
    "\n",
    "* **NaN/Inf Check (Not a Number / Infinity):**\n",
    "    * It checks the four main kinematic columns (`pt`, `eta`, `phi`, `mass`) simultaneously.\n",
    "    * `np.isfinite` determines if a number is neither $\\text{NaN}$ nor $\\pm\\infty$.\n",
    "    * `np.all(..., axis=1)` ensures that a row is kept **only if ALL four** values in that row are finite. This creates the `is_finite` mask. This values will cause issues in `vector` library.\n",
    "* **$p_T > 0$ Check:**\n",
    "    * It ensures that the transverse momentum is strictly positive (`pt_ok = df['pt'] > 0`). A lepton with $p_T \\le 0$ is physically irrelevant or indicative of a severe reconstruction failure.\n",
    "\n",
    "### 3. Final Filtering and Cleanup\n",
    "\n",
    "We combine the two validity checks (`is_finite` and `pt_ok`) into a single `valid_mask` and the DataFrame is filtered to keep only the valid rows (`df_cleaned = df[valid_mask]`).\n",
    "\n",
    "Finally, as with any filtering operation, it calls `.reset_index(drop=True)` to ensure the cleaned output DataFrame has a clean, continuous row index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kinematic_data(df):\n",
    "    \"\"\"\n",
    "    Corrects negative masses (reconstruction fault) and cleans invalid values\n",
    "    to ensure that 'vector.array' does not crash.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "    kinematic_cols = ['pt', 'eta', 'phi', 'mass']\n",
    "\n",
    "    # 1. CORRECTION: Identify and correct negative masses (the main problem)\n",
    "    negative_mass_mask = df['mass'] < 0\n",
    "    num_neg_mass = negative_mass_mask.sum()\n",
    "\n",
    "    if num_neg_mass > 0:\n",
    "        # Warning and correction: force unphysical masses to 0.0\n",
    "        print(f\"\\n--- MASS CORRECTION: {num_neg_mass} negative masses forced to 0.0 ---\")\n",
    "        df.loc[negative_mass_mask, 'mass'] = 0.1\n",
    "\n",
    "    # 2. CLEANUP: Mask for NaN/Inf (all columns must be finite)\n",
    "    is_finite = np.all(np.isfinite(df[kinematic_cols].values), axis=1)\n",
    "\n",
    "    # 3. CLEANUP: Mask for pT > 0 (mass is now >= 0 thanks to step 1)\n",
    "    pt_ok = df['pt'] > 0\n",
    "\n",
    "    # Combined mask for valid data that remains inside\n",
    "    valid_mask = is_finite & pt_ok\n",
    "\n",
    "    df_cleaned = df[valid_mask].reset_index(drop=True)\n",
    "    removed_count = initial_count - len(df_cleaned)\n",
    "\n",
    "    if removed_count > 0:\n",
    "        print(f\"-> Final cleanup after correction: {removed_count} rows rejected (NaN, Inf, or pT <= 0).\")\n",
    "    else:\n",
    "        print(\"-> Final cleanup: No invalid data found after mass correction.\")\n",
    "\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12362891",
   "metadata": {},
   "source": [
    "This function is a **Visual Diagnostics Tool** crucial for validating the filtering steps (like the `apply_quality_cuts` function)\n",
    "\n",
    "It takes two DataFrames: `df_before` (the raw, unfiltered lepton data) and `df_after` (the high-quality lepton data resulting from the cuts).\n",
    "\n",
    "\n",
    "This function serves as a **key quality assurance step**, providing clear visual evidence of the data selection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e344b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kinematic_diagnostics(df_before, df_after):\n",
    "    \"\"\"\n",
    "    Plots pT and eta distributions to visualize the effect of the applied cuts.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # --- Plot pT Distribution ---\n",
    "    axes[0].hist(df_before['pt'], bins=50, range=(0, 100), histtype='step',\n",
    "                 label='Before cuts', color='blue', density=True)\n",
    "    axes[0].hist(df_after['pt'], bins=50, range=(0, 100), histtype='stepfilled', alpha=0.6,\n",
    "                 label=f'After cuts ({len(df_after)} leptons)', color='red', density=True)\n",
    "\n",
    "    # Vertical line to show the pT > 5 GeV cut\n",
    "    axes[0].axvline(5.0, color='red', linestyle='--', linewidth=1, label='$p_T > 5 \\\\text{ GeV}$')\n",
    "\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].set_xlabel('$p_T$ of Lepton (GeV)')\n",
    "    axes[0].set_ylabel('Events (Normalized)')\n",
    "    axes[0].set_title('$p_T$ Distribution (Transverse Momentum)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.5)\n",
    "\n",
    "    # --- Plot eta Distribution ---\n",
    "    axes[1].hist(df_before['eta'], bins=40, range=(-3.0, 3.0), histtype='step',\n",
    "                 label='Before cuts', color='blue', density=True)\n",
    "    axes[1].hist(df_after['eta'], bins=40, range=(-3.0, 3.0), histtype='stepfilled', alpha=0.6,\n",
    "                 label=f'After cuts ({len(df_after)} leptons)', color='red', density=True)\n",
    "\n",
    "    # Vertical lines to show the |eta| < 2.5 cut\n",
    "    axes[1].axvline(2.5, color='red', linestyle='--', linewidth=1, label='$|\\\\eta| < 2.5$')\n",
    "    axes[1].axvline(-2.5, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    axes[1].set_xlabel('$\\\\eta$ of Lepton (Pseudorapidity)')\n",
    "    axes[1].set_ylabel('Events (Normalized)')\n",
    "    axes[1].set_title('$\\\\eta$ Distribution (Acceptance)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cddf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. MAIN EXECUTION ---\n",
    "print(\"--- START OF H -> 4l ANALYSIS (Real Data) ---\")\n",
    "\n",
    "# 1. Load the original data\n",
    "all_leptons_df_initial = load_and_flatten_data(DATA_FILES, MAX_EVENTS)\n",
    "\n",
    "if not all_leptons_df_initial.empty:\n",
    "    # 2. Apply individual quality cuts\n",
    "    filtered_leptons_df = apply_quality_cuts(all_leptons_df_initial)\n",
    "\n",
    "    cleaned_leptons_df = clean_kinematic_data(filtered_leptons_df)\n",
    "    # 4. CREATION OF LORENTZ VECTORS ON ALL FILTERED DATA (The right way)\n",
    "    print(\"\\nStep 4: Creation of Lorentz Vector (lv) objects on the complete filtered DataFrame...\")\n",
    "    try:\n",
    "        vector_array_temp = vector.array({\n",
    "            \"pt\": cleaned_leptons_df['pt'],\n",
    "            \"eta\": cleaned_leptons_df[\"eta\"],\n",
    "            \"phi\": cleaned_leptons_df[\"phi\"],\n",
    "            \"mass\": cleaned_leptons_df[\"mass\"]\n",
    "        })\n",
    "\n",
    "        cleaned_leptons_df['lv'] = vector_array_temp\n",
    "\n",
    "\n",
    "        print(\"Success: 'lv' column of Lorentz vectors added to the DataFrame.\")\n",
    "        print(cleaned_leptons_df['lv'].values.dtype)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCRITICAL ERROR during creation of 'lv' column: {e}\")\n",
    "        print(\"The analysis cannot continue. Please check the installation or version of 'vector'.\")\n",
    "\n",
    "\n",
    "    # 3. VERIFICATION: Display diagnostics\n",
    "    plot_kinematic_diagnostics(all_leptons_df_initial, cleaned_leptons_df)\n",
    "\n",
    "    # 4. Confirm the next step\n",
    "    print(\"\\n--- STATUS: Quality cuts applied and verified. ---\")\n",
    "\n",
    "    if not cleaned_leptons_df.empty:\n",
    "        print(\"\\nPreview of filtered leptons (ready for combination):\")\n",
    "        print(cleaned_leptons_df[['event_id', 'pt', 'charge', 'flavor', 'iso']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
